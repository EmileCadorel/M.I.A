mod mia::syntax::tokenizer;

import std::collection::map;
import std::collection::vec;
import std::io, std::stream;

/**
 * A tokenizer is an enhanced string splitter, that splits strings using tokens instead of just chars
 * Tokenizer are really usefull for grammar visitor, and can be associated with Lexers pretty easily
 * @example: 
 * ============
 * // using a tokenizer, tokens can be multiple letter long, and there can be collision between tokens
 * // For example, the token '=' and '=>' won't be a problem for the tokenizer
 * let dmut tzer = Tokenizer::new (tokens-> ["(", ")", "=>", ",", "=", "+", "*"]);
 * // set a skip token
 * tzer:.insert (" ", isSkip-> true); 
 *
 * let mut cursor = 0u64;
 * let str = "(x, y) => x + y * 2";
 * loop {
 *    let (len, isSkip) = tzer.next (str [cursor .. $]);
 *    if len != 0u64 {
 *        println (format ("%(y) -> %(b), %(r)", str [cursor .. cursor + len], isSkip));
 *    } else break {}
 * }
 * ============
 */
pub class @final Tokenizer {

    prv let dmut _heads = HashMap!{c32, &internal::Node}::new ();

    /**
     * Create a new tokenizer, with a set of tokens
     * @params: 
     *   - tokens: the list of token that will split the string
     * @example: 
     * ============
     * let dmut tzer = Tokenizer::new (tokens-> ["(", ")", "=>", ":", "<", ">", ",", " "]);
     * let str = "(x, y) => x > y";
     * let lst = tzer.tokenize (str);
     * assert (lst == ["(", "x", ",", " ", "y", ")", " ", "=>", " ", "x", " ", ">", " ", "y"]); 
     * ============
     */
    pub self (tokens: [[c32]] = []) {
        for i in tokens {
            self:.insert (i, isSkip-> false);
        }
    }
    
    /**
     * Insert a new token in the tokenizer
     * @params: 
     *    - token: the token to insert
     * @example: 
     * ================
     * let dmut tzer = Tokenizer::new ();
     * tzer:.insert ("+");
     * tzer:.insert ("+=");
     * tzer:.insert (" ");
     * let lst = tzer.tokenize ("x += y");
     * assert (lst == ["x", " ", "+=", " ", "y"]);
     * ================
     */
    pub def insert (mut self, token : [c32], isSkip : bool = false) {
        if (token.len != 0u64) {
            {               
                let fnd = (self._heads.find (token[0]));
                match fnd {
                    Ok (x:_) => {
                        (alias self._heads) [token[0]] = x.insert (token[1us..$], isSkip-> isSkip);                    
                    }
                    _ => {
                        (alias self._heads) [token[0]] = internal::Node::new (token[0], isSkip-> false).insert (token [1us..$], isSkip-> isSkip);
                    }
                }
            } catch {
                _ : &OutOfArray => { } // impossible
            }
        }
    }    
    
    /**
     * @returns: 
     *   - the length of the next token inside the str
     *   - true, if the token is a skip token false otherwise
     * @example: 
     * ============
     * let dmut tzer = Tokenizer::new (["+", " "]);
     * let mut str = "fst + scd";
     * let mut len = tzer.next (str)._0;
     * assert (len == 3us); // "fst"
     * 
     * str = str [len .. $];     
     * len = tzer.next (str)._0;
     * assert (len == 1us); // " "
     *
     * str = str [len .. $];     
     * len = tzer.next (str)._0;
     * assert (len == 1us); // "+"
     *
     * str = str [len .. $];     
     * len = tzer.next (str)._0;
     * assert (len == 1us); // " "
     *
     * str = str [len .. $];     
     * len = tzer.next (str)._0;
     * assert (len == 3us); // "scd" 
     *
     * str = str [len .. $];     
     * len = tzer.next (str)._0;
     * assert (len._0 == 0us); 
     * ============
     */
    pub def next (self, str : [c32])-> (usize, bool) {
        for i in 0us .. str.len {
            {
                let fnd = (self._heads.find (str [i]));
                match fnd { 
                    Ok (x:_) => { // a possible token at index == i
                        let (len, isSkip) = x.len (str [1us .. $]); // get the length of the token
                        // if the len is 0, then it is not really a token, it just start like one
                        if (len != 0us) {
                            if (i == 0us) { 
                                return (len, isSkip); // it is totally a token, we return its length
                            } else {
                                // it is a token, but there is something before it, so we return the len of the thing before it
                                return (i, false); 
                            }
                        }
                        // it was not a token, just started like one, we continue
                    }
                }
            } catch {
                _ : &OutOfArray=>  {
                    __pragma!panic ();
                } // impossible
            }
        }
        
        // No token in the str, return the len of the str
        return (str.len, false);
    }    

    impl Streamable;
    
}

mod internal {

    /**
     * A node of a tokenizer, that stores information about tokens
     */
    pub class @final Node {

        // The current value of the node
        let _key : c32; 

        // Can terminate a token? or is part of bigger tokens
        let _isToken : bool = false;

        // The list of possible continuation of the token
        let _heads : &HashMap!{c32, &Node};

        let _isSkip : bool;
        
        /**
         * Construct a new Token node
         * @params: 
         *   - key: the value of the node
         *   - isToken: can terminate a token 
         *   - heads: the list of possible continuation
         */
        pub self (key : c32, isToken : bool = false, heads : &HashMap!{c32, &Node} = {HashMap!{c32, &Node}::new ()}, isSkip : bool) with _key = key, _isToken = isToken, _heads = heads, _isSkip = isSkip
        {}
        
        /**
         * Insert sub tokens accepted tokens
         * @params: 
         *     - str: the rest to read to create a valid token
         * @example: 
         * ==============
         * // let say that "[+]" is a token, but "[" is not, nor is "[+"
         * let mut node = Node::new ('['); 
         * node = node.insert ("+]"); 
         * println (node); // [:false, +:false, ]:true 
         * // In that configuration the only token that will be accepted is "[+]"
         * assert (node.len ("[+]") == 3); // accepted
         * assert (node.len ("[") == 0us); // not accepted
         * assert (node.len ("[+") == 0us); // not accepted
         * 
         * // Now we want to accept "[-]"
         * node = node.insert ("-]");
         * // and simply "["
         * node = node.insert (""); 
         * println (node); // [:true, 
         *                 //     +:false, ]:true 
         *                 //     -: false, ]:true
         * 
         * assert (node.len ("[+]") == 3); // still accepted
         * assert (node.len ("[") == 1us); // accepted this time
         * assert (node.len ("[+") == 1us); // accept only the '['
         * assert (node.len ("[-]") == 3); // accepted
         * assert (node.len ("[-") == 1us); // accept only the '['
         * ==============
         */
        pub def insert (self, str : [c32], isSkip : bool) -> &Node {
            if (str.len == 0us) {
                return Node::new (self._key, isToken-> true, heads-> self._heads, isSkip-> isSkip)
            }

            let dmut retDict = HashMap!{c32, &Node}::new ();
            for i, j in self._heads {
                retDict:.insert (i, j);                    
            }

            {
                let fnd = retDict.find (str [0])
                    match fnd {
                        Ok (x:_) => {
                            retDict:.insert (str [0], x.insert (str [1us..$], isSkip));
                        }
                        _ => {                    
                            retDict:.insert (str [0], Node::new (str [0], false).insert (str [1us..$], isSkip));
                        }
                    }
            } catch {
                _: &OutOfArray => {}
            }
            
            return Node::new (self._key, isToken-> self._isToken, heads-> retDict, isSkip-> isSkip);
        }

        

        /**
         * @returns: the length of the token at the beginning of the string content
         * @example: 
         * =================
         * let mut node = Node::new ('+', isToken-> true);
         * node = node.insert ("=");
         * // Our grammar accept the tokens, "+" and "+="
         * assert (node.len ("+")._0 == 1us); // "+" are accepted
         * assert (node.len ("+=")._0 == 2us); // "+=" are accepted
         * assert (node.len (" +=")._0 == 0us); // " +=" are not accepted
         * assert (node.len ("+ and some rest")._0 == 1us); // " +" are accepted
         * =================
         */
        pub def len (self, content : [c32])-> (usize, bool) {
            if (content.len == 0us) {
                if (self._isToken)
                    return (1us, self._isSkip);
                else return (0us, false);
            }

            {
                let fnd = (self._heads.find (content [0]));
                match fnd {
                    Ok (x:_) => {
                        let (sub_len, isSkip) = x.len (content [1us..$]);
                        if (sub_len != 0us)
                            return (1us + sub_len, isSkip);
                    }
                }
            }  catch {
                _: &OutOfArray => {
                    __pragma!panic ();
                } // impossible
            }

            if (self._isToken)
                return (1us, self._isSkip);
            return (0us, false);
        }

        /**
         * @returns: the key of the node
         */
        def key (self) -> c32 {
            self._key
        }

        /**
         * @returns: true, if this token is a skip token
         */
        def isSkip (self)-> bool {
            self._isSkip
        }
        
    }

}
