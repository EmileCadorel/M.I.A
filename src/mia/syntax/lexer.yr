mod mia::syntax::lexer
    
import mia::syntax::tokenizer;
import std::io, std::stream;

import std::collection::vec;
import std::collection::map;
import std::collection::set; 
import mia::utils::colors;

/**
 * A word encode a content part of a file
 * It is used to identify part of a file as a token, keyword, identifier, etc.
 */
pub class @final Token {

    let _str : [c32];

    let _line : usize;

    let _col : usize;

    let _seek : usize;

    let _fileContent : [c32];

    let _lineSeek : usize;

    let _isEof : bool = false;

    /**
     * Create an empty word
     * @params: 
     *   - filename: the file in which we want an empty word
     */
    pub self eof () with
        _str = "", _line = 0us, _col = 0us, _seek = 0us, _fileContent = "", _lineSeek = 0us, _isEof = true
    {}

    /**
     * Create a word from a file 
     * @params: 
     *   - str: the content of the word
     *   - filename: the name of the file that contains the word
     *   - line: the location line of the word
     *   - col: the location column of the word
     *   - seek: the location cursor of the word (number of c32 preceding it)
     *   - lineSeek: the location cursor of the beginning of the line containing the word
     */
    pub self (str : [c32], fileContent : [c32], line : usize, col : usize, seek : usize, lineSeek : usize) with
        _str = str, _fileContent = fileContent, _line = line, _col = col, _seek = seek, _lineSeek = lineSeek
    {}

    /**
     * @returns: the content of the word
     */
    pub def str (self) -> [c32] {
        self._str
    }

    /**
     * @returns: the length of the content of the word
     */
    pub def len (self) -> usize {
        self._str.len 
    }

    /**
     * @returns: the location column of the word
     */
    pub def col (self) -> usize {
        self._col
    }

    /**
     * @returns: the location line of the word
     */
    pub def line (self) -> usize {
        self._line
    }

    /**
     * @returns: the content of the file containing the word
     */
    pub def fileContent (self) -> [c32] {
        self._fileContent
    }

    /**
     * @returns: the cursor position of the line containing the word
     */
    pub def lineSeek (self)-> usize {
        self._lineSeek
    }

    /**
     * @returns: true if the word is empty, (constructed with eof), false otherwise 
     */
    pub def isEof (self) -> bool {
        self._isEof
    }

    /**
     * @returns: true if the word is not empty, and its content is equals to o
     */
    pub def opEquals (self, o : [c32]) -> bool {
        if (self._isEof)
            false
        else 
            self._str == o
    }       
    
    impl Streamable {

        pub over toStream (self, dmut stream : &StringStream)-> void {
            stream:.write (typeof (self)::typeid, "(", Colors::YELLOW, self._str, Colors::RESET, ",", self._line, ",", self._col, ")");
        }
    }
    
}

pub class @final Lexer {

    let dmut _tzer : &Tokenizer;
    
    let _content : [c32];

    let dmut _skips = HashSet!{[c32]}::new ()
    
    let mut _line = 1us;

    let mut _col = 1us;

    let mut _cursor = 0us;

    let dmut _rewinders = Vec!{(usize, usize, usize, usize)}::new ();

    let dmut _lineSeek = 0us;

    let mut _eof : bool = false;

    /**
     * Create a new lexer, ready to split content
     * @params: 
     *    - content: the content to split
     *    - tokens: the list of tokens that split the string, (cf. Tokenizer)
     *    - skips: the list of tokens that will be omitted by the lexer when reading (by default [" ", "\n", "\t", "\r"])
     * @warning: if the skips token are not in tokens, they are added, so they split the content 
     */
    pub self (content : [c32], tokens: [[c32]] = [" "], skips: [[c32]] = [" ", "\n", "\t", "\r"])
        with _tzer = Tokenizer::new (tokens-> tokens),
             _content = content
    {        
        self._rewinders:.push ((self._cursor, self._line, self._col, self._lineSeek));

        for i in skips {
            self._skips:.insert (i);
            self._tzer:.insert (i, isSkip-> true);
        }
    }        

    pub def setSkips (mut self, skips : [[c32]]) {
        for i in self._skips {
            self._tzer:.insert (i, isSkip-> false);
        }
        
        self._skips:.clear ();
        
        for i in skips {
            self._skips:.insert (i);
            self._tzer:.insert (i, isSkip-> true);
        }
    }

    /**
     * Read the next word in the lexer, and returns it
     */
    pub def next (mut self)-> &Token {
        if (self._eof) return Token::eof ();
        
        self._rewinders:.push ((self._cursor, self._line, self._col, self._lineSeek));
        return loop {
            let (wd, isSkip) = self._tzer.next (self._content [self._cursor .. $]);
            if (wd != 0u64) {
                let ret = self._content [self._cursor .. self._cursor + wd];
                self._cursor += wd;
                let (old_l, old_c) = (self._line, self._col);
                self:.incrementLine (ret);
                if (!isSkip) {
                    return Token::new (str-> ret, fileContent-> self._content, old_l, old_c, self._cursor - wd, self._lineSeek);                    
                }                
            } else {
                self._eof = true;
                break Token::eof ();
            }
        };
    } catch {
        _ => {
            self._eof = true;
            return Token::eof ();
        }
    }

    /**
     * Increment the line, col, and lineSeek counters
     */
    prv def incrementLine (mut self, txt : [c32]) {
        for i in txt {
            if (i == '\n'c32) {
                self._lineSeek = self._lineSeek + self._col;
                self._line += 1us;
                self._col = 1us;
            } else {
                self._col += 1us;
            }
        }
    }

    /**
     * Rewind to a previous location in the file
     * Each time the function next is called, the lexer saves the cursor position
     * This function rewind go to that cursor position, so if we rewind 10 times, we will get the last 10 next calls
     * @warning: 
     * ===============
     * when toggeling skips, and doComment, the rewind does not garantee, that the lexer will return at least nb tokens
     * ===============
     */
    pub def rewind (mut self, nb : usize = 1us) {
        if (self._eof) {
            self._eof = false;
        }
        
        for _ in 0us .. nb {
            {
                let (x, y, z, w) = self._rewinders [self._rewinders.len () - 1us] ;
                self._cursor = x;
                self._line = y;
                self._col = z;
                self._lineSeek = w;
                self._rewinders:.pop ();
            }?;
        };

        if (self._rewinders.len () == 0us) {
            self._rewinders:.push ((self._cursor, self._line, self._col, self._lineSeek));
        }
    }
    
    /**
     * @returns: the counter of next call
     * @info: this information can be usefull to go back rapidely
     * @example: 
     * =================
     * let dmut lex : &Lexer = ...
     * let read_Nb : usize = ...
     * let counter = lex.getCounter ();
     * for i in 0us .. read_Nb {
     *     println (lex:.next ()._0);
     * }
     * lex:.rewind (nb-> lex.getCounter () - counter); // go back of number of valid read done
     * // so the lexer will be reset, as if the for loop never executed
     * =================
     */
    pub def getCounter (self)-> usize {
        self._rewinders.len ()
    }


    pub def isEof (self)-> bool {
        self._eof
    }

    impl Streamable;
    
}



